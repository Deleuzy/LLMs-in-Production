{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iSIIb6ey0POS"
      },
      "outputs": [],
      "source": [
        "# Install compatible versions\n",
        "!pip install -q \\\n",
        "    \"numpy<2\" \\\n",
        "    \"chromadb==0.4.18\" \\\n",
        "    \"llama-index==0.9.14.post3\" \\\n",
        "    \"openai>=1.12.0\" \\\n",
        "    \"httpx>=0.25.0,<0.27\" \\\n",
        "    \"tiktoken\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "os.environ['ACTIVELOOP_TOKEN'] = '<YOUR_ACTIVELOOP_API_KEY>'\n",
        "os.environ['COHERE_API_KEY'] = 'CbjEmGRTV5xDsKH23SAZobiBAF9BOegHjBjoWjeu'"
      ],
      "metadata": {
        "id": "Ul1SFfON0TD1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p './paul_graham/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O './paul_graham/paul_graham_essay.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7rXjNi00bWW",
        "outputId": "204f1cff-cbd8-4186-fbd0-967617458883"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-03 17:32:40--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘./paul_graham/paul_graham_essay.txt’\n",
            "\n",
            "\r          ./paul_gr   0%[                    ]       0  --.-KB/s               \r./paul_graham/paul_ 100%[===================>]  73.28K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-12-03 17:32:41 (4.97 MB/s) - ‘./paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"./paul_graham\").load_data()"
      ],
      "metadata": {
        "id": "C1Q0cMtj0kOs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"false\""
      ],
      "metadata": {
        "id": "dubRJH9_Rirt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Chunk ---\n",
        "from llama_index import ServiceContext\n",
        "service_context = ServiceContext.from_defaults(chunk_size=512, chunk_overlap=64)\n",
        "nodes = service_context.node_parser.get_nodes_from_documents(documents)\n",
        "\n"
      ],
      "metadata": {
        "id": "h9CPwNm10vj2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import SimpleDirectoryReader, ServiceContext, VectorStoreIndex\n",
        "from llama_index.vector_stores import ChromaVectorStore\n",
        "from llama_index.storage.storage_context import StorageContext\n",
        "\n",
        "# Load & chunk\n",
        "documents = SimpleDirectoryReader(\"./paul_graham\").load_data()\n",
        "service_context = ServiceContext.from_defaults(chunk_size=512)\n",
        "nodes = service_context.node_parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# Chroma + index\n",
        "chroma_client = chromadb.EphemeralClient()\n",
        "chroma_collection = chroma_client.create_collection(\"paul_graham\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "index = VectorStoreIndex(nodes, storage_context=storage_context, service_context=service_context)\n",
        "print(\"✅ Success!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2udbSiZParm",
        "outputId": "0f4cff05-4b10-4b00-ea34-20ff0233e6fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"false\""
      ],
      "metadata": {
        "id": "sPhMqROgR-ns"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(streaming=True, similarity_top_k=5)\n",
        "response = query_engine.query(\"What does Paul Graham do?\")\n",
        "response.print_response_stream()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD_K2pMvR6Za",
        "outputId": "53ba1f11-99ff-4b60-bf0e-8cf1e806b882"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paul Graham is involved in various activities such as running Y Combinator, working on coding projects like Bel, writing essays, hosting dinners, and investing in startups through programs like the Summer Founders Program."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.query_engine import SubQuestionQueryEngine\n",
        "\n",
        "subq_engine = SubQuestionQueryEngine.from_defaults(\n",
        "    query_engine_tools=[tool],\n",
        "    service_context=service_context,\n",
        "    use_async=False  # ← critical for Colab/Jupyter\n",
        ")"
      ],
      "metadata": {
        "id": "PYkxLxJ8S9PD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.query_engine import SubQuestionQueryEngine\n",
        "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.callbacks import CallbackManager\n",
        "import logging\n",
        "\n",
        "# Enable LlamaIndex debug logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Wrap your query engine as a tool (must have a name!)\n",
        "tool = QueryEngineTool(\n",
        "    query_engine=query_engine,\n",
        "    metadata=ToolMetadata(\n",
        "        name=\"pg_essay\",  # ← this name appears in logs like [pg_essay]\n",
        "        description=\"Essay by Paul Graham titled 'What I Worked On'\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Create sub-question engine WITH verbose logging\n",
        "subq_engine = SubQuestionQueryEngine.from_defaults(\n",
        "    query_engine_tools=[tool],\n",
        "    service_context=service_context,\n",
        "    use_async=False,\n",
        "    verbose=True  # ← this enables the step-by-step Q&A printout\n",
        ")\n",
        "\n",
        "# Run query\n",
        "response = subq_engine.query(\"How was Paul Graham's life different before, during, and after YC?\")\n",
        "print(\"\\n>>> The final response:\\n\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNGkOdYJSQy0",
        "outputId": "89b396a4-c24a-48cd-a958-867ed9e1abd7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 3 sub questions.\n",
            "\u001b[1;3;38;2;237;90;200m[pg_essay] Q: What did Paul Graham work on before YC?\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200m[pg_essay] A: Before YC, Paul Graham worked on writing essays and hacking.\n",
            "\u001b[0m\u001b[1;3;38;2;90;149;237m[pg_essay] Q: What did Paul Graham work on during YC?\n",
            "\u001b[0m\u001b[1;3;38;2;90;149;237m[pg_essay] A: Paul Graham worked on writing essays and working on Y Combinator (YC) during YC.\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203m[pg_essay] Q: What did Paul Graham work on after YC?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203m[pg_essay] A: Paul Graham worked on painting after YC.\n",
            "\u001b[0m\n",
            ">>> The final response:\n",
            " Paul Graham's life involved writing essays and hacking before Y Combinator (YC), during YC he focused on writing essays and working on YC itself, and after YC he shifted his focus to painting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "    \"numpy<2\" \\\n",
        "    \"chromadb==0.4.18\" \\\n",
        "    \"llama-index==0.9.14.post3\" \\\n",
        "    \"openai>=1.12.0\" \\\n",
        "    \"tiktoken\""
      ],
      "metadata": {
        "id": "uTLn5cPXVdMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 2. Setup\n",
        "# ==============================\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"false\"\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# ==============================\n",
        "# 3. Load Paul Graham Essay\n",
        "# ==============================\n",
        "!mkdir -p ./paul_graham/\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O './paul_graham/paul_graham_essay.txt'\n",
        "\n",
        "from llama_index import SimpleDirectoryReader\n",
        "document = SimpleDirectoryReader(\"./paul_graham\").load_data()\n",
        "print(f\"Loaded {len(document[0].text)} characters.\")\n",
        "\n",
        "# ==============================\n",
        "# 4. Chunk with larger context + proper LLM\n",
        "# ==============================\n",
        "from llama_index import ServiceContext\n",
        "from llama_index.llms import OpenAI  # ← IMPORT THIS\n",
        "from llama_index.embeddings import OpenAIEmbedding # ← ADD THIS IMPORT\n",
        "\n",
        "# ✅ CORRECT LLM INITIALIZATION\n",
        "llm = OpenAI(\n",
        "    model=\"gpt-3.5-turbo\", # Changed from gpt-4o-mini to gpt-3.5-turbo\n",
        "    temperature=0.3,      # slight creativity for richer answers\n",
        "    max_tokens=1000       # ← critical for long responses\n",
        ")\n",
        "\n",
        "# Instantiate the embedding model\n",
        "embed_model = OpenAIEmbedding(model_name=\"text-embedding-ada-002\") # ← ADD THIS LINE\n",
        "\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=llm,              # ← pass LLM object, NOT string\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=128,\n",
        "    embed_model=embed_model # ← pass the instantiated embedding model\n",
        ")\n",
        "\n",
        "nodes = service_context.node_parser.get_nodes_from_documents(document)\n",
        "print(f\"Created {len(nodes)} chunks.\")\n",
        "\n",
        "# ==============================\n",
        "# 5. Chroma Vector Store\n",
        "# ==============================\n",
        "import chromadb\n",
        "from llama_index.vector_stores import ChromaVectorStore\n",
        "from llama_index.storage.storage_context import StorageContext\n",
        "from llama_index import VectorStoreIndex\n",
        "\n",
        "chroma_client = chromadb.EphemeralClient()\n",
        "chroma_collection = chroma_client.create_collection(\"paul_graham\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "index = VectorStoreIndex(\n",
        "    nodes,\n",
        "    storage_context=storage_context,\n",
        "    service_context=service_context\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# 6. Query Engine + SubQuestion (VERBOSE)\n",
        "# ==============================\n",
        "query_engine = index.as_query_engine(similarity_top_k=5)\n",
        "\n",
        "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.query_engine import SubQuestionQueryEngine\n",
        "\n",
        "tool = QueryEngineTool(\n",
        "    query_engine=query_engine,\n",
        "    metadata=ToolMetadata(\n",
        "        name=\"pg_essay\",\n",
        "        description=\"Paul Graham's full essay 'What I Worked On'\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "subq_engine = SubQuestionQueryEngine.from_defaults(\n",
        "    query_engine_tools=[tool],\n",
        "    service_context=service_context,\n",
        "    use_async=False,\n",
        "    verbose=True  # Shows [pg_essay] Q/A steps\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# 7. Run query\n",
        "# ==============================\n",
        "question = \"How was Paul Graham's life different before, during, and after YC?\"\n",
        "print(f\"❓ {question}\\n{'-'*60}\")\n",
        "\n",
        "response = subq_engine.query(question)\n",
        "\n",
        "print(\"\\n>>> The final response:\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT7ivNhbUEuf",
        "outputId": "c1fa4866-b371-4254-fbaf-e1a65f84180f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-03 17:49:19--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘./paul_graham/paul_graham_essay.txt’\n",
            "\n",
            "\r          ./paul_gr   0%[                    ]       0  --.-KB/s               \r./paul_graham/paul_ 100%[===================>]  73.28K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-12-03 17:49:19 (5.04 MB/s) - ‘./paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n",
            "Loaded 75014 characters.\n",
            "Created 20 chunks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❓ How was Paul Graham's life different before, during, and after YC?\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 3 sub questions.\n",
            "\u001b[1;3;38;2;237;90;200m[pg_essay] Q: What did Paul Graham work on before YC?\n",
            "\u001b[0m\u001b[1;3;38;2;237;90;200m[pg_essay] A: Paul Graham worked on spam filters, did some painting, hosted dinners for friends, and bought a building in Cambridge to use as an office before starting Y Combinator.\n",
            "\u001b[0m\u001b[1;3;38;2;90;149;237m[pg_essay] Q: What did Paul Graham work on during YC?\n",
            "\u001b[0m\u001b[1;3;38;2;90;149;237m[pg_essay] A: During YC, Paul Graham worked on various projects including writing essays, working on YC's internal software in Arc, and dealing with the day-to-day operations and challenges faced by startups in the program.\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203m[pg_essay] Q: What did Paul Graham work on after YC?\n",
            "\u001b[0m\u001b[1;3;38;2;11;159;203m[pg_essay] A: After Y Combinator, Paul Graham worked on Bel, a project that was hard but satisfying. He also moved to England with his family in the summer of 2016, where most of Bel was written. In the fall of 2019, Bel was finally finished, resembling a spec rather than an implementation, similar to McCarthy's original Lisp.\n",
            "\u001b[0m\n",
            ">>> The final response:\n",
            "Paul Graham's life was focused on working on spam filters, painting, hosting dinners for friends, and buying a building in Cambridge before starting Y Combinator. During Y Combinator, he worked on various projects such as writing essays, developing YC's internal software in Arc, and handling the day-to-day operations of startups in the program. After Y Combinator, he worked on Bel, a challenging but rewarding project, and eventually moved to England with his family to continue working on Bel, which was completed in a manner resembling a spec rather than an implementation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cohere Rerank"
      ],
      "metadata": {
        "id": "t8jeNI3Igwqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "\n",
        "# Get your cohere API key on: www.cohere.com\n",
        "co = cohere.Client(os.environ['COHERE_API_KEY'])\n",
        "\n",
        "# Example query and passages\n",
        "query = \"What is the capital of the United States?\"\n",
        "documents = [\n",
        "   \"Carson City is the capital city of the American state of Nevada. At the  2010 United States Census, Carson City had a population of 55,274.\",\n",
        "   \"The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean that are a political division controlled by the United States. Its capital is Saipan.\",\n",
        "   \"Charlotte Amalie is the capital and largest city of the United States Virgin Islands. It has about 20,000 people. The city is on the island of Saint Thomas.\",\n",
        "   \"Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district. \",\n",
        "   \"Capital punishment (the death penalty) has existed in the United States since before the United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states.\",\n",
        "   \"North Dakota is a state in the United States. 672,591 people lived in North Dakota in the year 2010. The capital and seat of government is Bismarck.\"\n",
        "   ]"
      ],
      "metadata": {
        "id": "mVJOtIHQgxlf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = co.rerank(query=query, documents=documents, top_n=3, model='rerank-english-v3.0') # Change top_n to change the number of results returned. If top_n is not passed, all results will be returned.\n",
        "\n",
        "for idx, r in enumerate(results):\n",
        "  print(f\"Document Rank: {idx + 1}, Document Index: {r.index}\")\n",
        "  print(f\"Document: {r.document['text']}\")\n",
        "  print(f\"Relevance Score: {r.relevance_score:.2f}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0nOLQmmg3yY",
        "outputId": "f39794cd-0785-4bce-b37c-95d32504973c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Rank: 1, Document Index: 3\n",
            "Document: Washington, D.C. (also known as simply Washington or D.C., and officially as the District of Columbia) is the capital of the United States. It is a federal district. \n",
            "Relevance Score: 1.00\n",
            "\n",
            "\n",
            "Document Rank: 2, Document Index: 4\n",
            "Document: Capital punishment (the death penalty) has existed in the United States since before the United States was a country. As of 2017, capital punishment is legal in 30 of the 50 states.\n",
            "Relevance Score: 0.78\n",
            "\n",
            "\n",
            "Document Rank: 3, Document Index: 1\n",
            "Document: The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean that are a political division controlled by the United States. Its capital is Saipan.\n",
            "Relevance Score: 0.09\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cohere in LlamaIndex"
      ],
      "metadata": {
        "id": "bifEKCqihBBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
        "\n",
        "\n",
        "cohere_rerank = CohereRerank(api_key=os.environ['COHERE_API_KEY'], model='rerank-english-v3.0', top_n=2)"
      ],
      "metadata": {
        "id": "AtRoHfgClgqS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=10,\n",
        "    node_postprocessors=[cohere_rerank],\n",
        ")"
      ],
      "metadata": {
        "id": "qpU4Qwo3lgns"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"What did Sam Altman do in this essay?\",\n",
        ")\n",
        "print( response )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uvVyML8lgkx",
        "outputId": "e79f34b9-5578-4862-875d-25499239e677"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sam Altman was asked to become the president of Y Combinator after Paul Graham decided to step down from running the organization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9rXR7WjalgiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QDx16J7Flgfa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}