{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hRLYtDt3RdIa"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-openai openai python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"OPENAI_API_KEY=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\" > .env"
      ],
      "metadata": {
        "id": "MMR5OA00RjjQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser\n",
        "\n",
        "# ============================================================================\n",
        "# 1. BASIC CALLING\n",
        "# ============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"1. BASIC CALLING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "prompt = ChatPromptTemplate.from_template(\"What is a word to replace the following: {word}?\")\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Single call\n",
        "result = chain.invoke({\"word\": \"artificial\"})\n",
        "print(f\"\\nSingle call result: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYrRuyr-Rvmq",
        "outputId": "52594905-ce1c-40d9-f077-e20d31857c6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "1. BASIC CALLING\n",
            "============================================================\n",
            "\n",
            "Single call result: Synthetic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"2. BATCH - Process multiple inputs\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "input_list = [\n",
        "    {\"word\": \"artificial\"},\n",
        "    {\"word\": \"intelligence\"},\n",
        "    {\"word\": \"robot\"}\n",
        "]\n",
        "\n",
        "# Modern way: use .batch() instead of .apply()\n",
        "batch_results = chain.batch(input_list)\n",
        "print(f\"\\nBatch results: {batch_results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtw--S5YR9YY",
        "outputId": "5af91d25-3371-4c5f-b708-3348363d3864"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "2. BATCH - Process multiple inputs\n",
            "============================================================\n",
            "\n",
            "Batch results: ['synthetic', 'Cleverness', 'android']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get raw results (AIMessage objects)\n",
        "raw_chain = prompt | llm\n",
        "batch_results = raw_chain.batch(input_list)\n",
        "\n",
        "# Now you can access metadata\n",
        "first = batch_results[0]\n",
        "print(\"Text:\", first.content)\n",
        "print(\"Model used:\", first.response_metadata.get(\"model_name\", \"unknown\"))\n",
        "print(\"Token usage:\", getattr(first, \"usage_metadata\", \"N/A\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTXuwpqeSntB",
        "outputId": "01003260-c1fc-49f3-ea55-7beb23fb22a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Synthetic\n",
            "Model used: gpt-3.5-turbo-0125\n",
            "Token usage: {'input_tokens': 18, 'output_tokens': 2, 'total_tokens': 20, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"3. MULTIPLE INPUT VARIABLES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "prompt_multi = ChatPromptTemplate.from_template(\n",
        "    \"Looking at the context of '{context}'. What is an appropriate word to replace the following: {word}?\"\n",
        ")\n",
        "chain_multi = prompt_multi | llm | StrOutputParser()\n",
        "\n",
        "result1 = chain_multi.invoke({\"word\": \"fan\", \"context\": \"object\"})\n",
        "print(f\"\\nContext 'object': {result1}\")\n",
        "\n",
        "result2 = chain_multi.invoke({\"word\": \"fan\", \"context\": \"humans\"})\n",
        "print(f\"Context 'humans': {result2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0FxnT-9Ta3w",
        "outputId": "280ec87e-a36c-452c-bca0-0e6059ac7e12"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "3. MULTIPLE INPUT VARIABLES\n",
            "============================================================\n",
            "\n",
            "Context 'object': air conditioner\n",
            "Context 'humans': supporter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"4. OUTPUT PARSERS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "list_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "prompt_with_parser = ChatPromptTemplate.from_template(\n",
        "    \"List all possible words as substitute for 'artificial' as comma separated. {format_instructions}\"\n",
        ")\n",
        "\n",
        "chain_parsed = prompt_with_parser | llm | list_parser\n",
        "\n",
        "parsed_result = chain_parsed.invoke({\n",
        "    \"format_instructions\": list_parser.get_format_instructions()\n",
        "})\n",
        "print(f\"\\nParsed list result: {parsed_result}\")\n",
        "print(f\"Type: {type(parsed_result)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr4D8o5KTvEy",
        "outputId": "c666cb6d-f80d-47ed-fde9-05293e891c54"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "4. OUTPUT PARSERS\n",
            "============================================================\n",
            "\n",
            "Parsed list result: ['synthetic', 'man-made', 'fake', 'simulated', 'imitation', 'faux', 'ersatz', 'fabricated', 'manufactured', 'unnatural']\n",
            "Type: <class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-openai langchain-community langchain-core"
      ],
      "metadata": {
        "id": "5nFTIRDdWc1X"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 5. CONVERSATIONAL CHAIN WITH MEMORY - FIXED\n",
        "# ============================================================================\n",
        "print(\"=\"*60)\n",
        "print(\"5. CONVERSATIONAL CHAIN WITH MEMORY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Updated imports for modern LangChain\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Create the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# Create prompt with memory placeholder\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant that suggests word alternatives.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Create the chain\n",
        "chain = prompt | llm\n",
        "\n",
        "# Add memory to the chain\n",
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "chain_with_memory = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: chat_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\",\n",
        ")\n",
        "\n",
        "# First turn\n",
        "first_response = chain_with_memory.invoke(\n",
        "    {\"input\": \"List 3 words that can replace 'artificial'. Be brief.\"},\n",
        "    config={\"configurable\": {\"session_id\": \"session1\"}}\n",
        ")\n",
        "print(f\"\\nFirst turn: {first_response.content}\")\n",
        "\n",
        "# Second turn - remembers the first!\n",
        "second_response = chain_with_memory.invoke(\n",
        "    {\"input\": \"And the next 4?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"session1\"}}\n",
        ")\n",
        "print(f\"Second turn: {second_response.content}\")\n",
        "\n",
        "# Third turn\n",
        "third_response = chain_with_memory.invoke(\n",
        "    {\"input\": \"Give me 2 more\"},\n",
        "    config={\"configurable\": {\"session_id\": \"session1\"}}\n",
        ")\n",
        "print(f\"Third turn: {third_response.content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6vJCC7lVnDc",
        "outputId": "fee17849-d6c0-4a39-a8c5-91dbeeb82f2f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "5. CONVERSATIONAL CHAIN WITH MEMORY\n",
            "============================================================\n",
            "\n",
            "First turn: 1. Synthetic\n",
            "2. Man-made\n",
            "3. Fake\n",
            "Second turn: 4. Faux\n",
            "5. Imitation\n",
            "6. Simulated\n",
            "7. Manufactured\n",
            "Third turn: 8. Replicated\n",
            "9. Counterfeit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"7. SEQUENTIAL CHAINS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define first chain: get meaning\n",
        "prompt1 = ChatPromptTemplate.from_template(\n",
        "    \"What is the meaning of the word '{word}'? Answer in one sentence.\"\n",
        ")\n",
        "chain1 = prompt1 | llm | StrOutputParser()\n",
        "\n",
        "# Define second chain: get synonym\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"What is a word to replace '{word}'? Give just one word.\"\n",
        ")\n",
        "chain2 = prompt2 | llm | StrOutputParser()\n",
        "\n",
        "# Run them sequentially (manually)\n",
        "word = \"artificial\"\n",
        "meaning = chain1.invoke({\"word\": word})\n",
        "synonym = chain2.invoke({\"word\": word})\n",
        "\n",
        "print(f\"\\nWord: {word}\")\n",
        "print(f\"Meaning: {meaning}\")\n",
        "print(f\"Synonym: {synonym}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyK_os36Xxcg",
        "outputId": "885d5f32-cf23-44ef-b7a1-6df4427ac9e8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "7. SEQUENTIAL CHAINS\n",
            "============================================================\n",
            "\n",
            "Word: artificial\n",
            "Meaning: Artificial means something made or produced by human beings rather than occurring naturally.\n",
            "Synonym: Synthetic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"8. CUSTOM CHAIN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "def concatenate_outputs(word):\n",
        "    \"\"\"Custom function that gets meaning and synonym, then concatenates\"\"\"\n",
        "    meaning = chain1.invoke({\"word\": word})\n",
        "    synonym = chain2.invoke({\"word\": word})\n",
        "    return f\"MEANING: {meaning}\\n\\nSYNONYM: {synonym}\"\n",
        "\n",
        "# Wrap custom function in a Runnable\n",
        "custom_chain = RunnableLambda(concatenate_outputs)\n",
        "\n",
        "custom_result = custom_chain.invoke(\"artificial\")\n",
        "print(f\"\\nCustom chain result:\\n{custom_result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cCXQMSbZELK",
        "outputId": "102ee0d1-fe68-4a1b-b2f5-a6253ab83af5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "8. CUSTOM CHAIN\n",
            "============================================================\n",
            "\n",
            "Custom chain result:\n",
            "MEANING: Artificial means made or produced by human beings rather than occurring naturally.\n",
            "\n",
            "SYNONYM: Synthetic\n"
          ]
        }
      ]
    }
  ]
}