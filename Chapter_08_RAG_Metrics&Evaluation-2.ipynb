{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oEWV3BrvfefQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02cc2772-7850-40b4-b160-5b6b96cada45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama-cloud                              0.1.19\n",
            "llama-index                              0.9.14.post3\n",
            "llama-index-cli                          0.1.13\n",
            "llama-index-instrumentation              0.4.2\n",
            "llama-index-multi-modal-llms-openai      0.1.9\n",
            "llama-index-readers-llama-parse          0.1.6\n",
            "llama-index-workflows                    2.11.5\n",
            "llama-parse                              0.4.9\n",
            "llamaindex-py-client                     0.1.19\n",
            "\u001b[33mWARNING: Skipping llama-index-indices-managed-llama-cloud as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping llama-cloud-services as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# STOP FIGHTING - USE WHAT COLAB HAS\n",
        "# ============================================\n",
        "\n",
        "# First, check what's already installed\n",
        "!pip list | grep llama\n",
        "\n",
        "# Uninstall the problematic packages ONLY\n",
        "!pip uninstall -y llama-index-indices-managed-llama-cloud llama-cloud-services\n",
        "\n",
        "# Install ONLY what we need on top of what's there\n",
        "!pip install -q deeplake ragas==0.0.22 html2text==2020.1.16 nest-asyncio\n",
        "\n",
        "# DON'T restart - just run this:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# FINAL FIX - Update OpenAI to work with httpx\n",
        "# ============================================\n",
        "\n",
        "!pip install --upgrade openai\n",
        "\n",
        "# NO RESTART NEEDED - Just re-run the imports:"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMGE8VnJVqjk",
        "outputId": "d38fa094-15cd-4b7b-cf54-945e627dc3b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import nest_asyncio\n",
        "asyncio = __import__('asyncio')\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Set API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# Try modern imports first\n",
        "try:\n",
        "    from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, StorageContext, Settings\n",
        "    from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
        "    from llama_index.llms.openai import OpenAI\n",
        "    from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "    from llama_index.core.node_parser import SimpleNodeParser\n",
        "    from llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator\n",
        "    from llama_index.readers.web import SimpleWebPageReader\n",
        "    print(\"‚úÖ Using MODERN llama-index API\")\n",
        "    MODERN_API = True\n",
        "except ImportError:\n",
        "    # Fall back to legacy imports\n",
        "    from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, StorageContext\n",
        "    from llama_index.vector_stores import DeepLakeVectorStore\n",
        "    from llama_index.llms import OpenAI\n",
        "    from llama_index.node_parser import SimpleNodeParser\n",
        "    from llama_index.evaluation import FaithfulnessEvaluator, RelevancyEvaluator\n",
        "    from llama_index.readers.web import SimpleWebPageReader\n",
        "    print(\"‚úÖ Using LEGACY llama-index API\")\n",
        "    MODERN_API = False\n",
        "\n",
        "# Now setup LLM\n",
        "llm = OpenAI(model=\"gpt-4\", temperature=0.0)\n",
        "\n",
        "if MODERN_API:\n",
        "    # Modern API uses Settings\n",
        "    Settings.llm = llm\n",
        "    Settings.embed_model = OpenAIEmbedding()\n",
        "    service_context = None\n",
        "else:\n",
        "    # Legacy API uses ServiceContext\n",
        "    service_context = ServiceContext.from_defaults(llm=llm)\n",
        "\n",
        "print(\"‚úÖ LLM initialized\")\n",
        "\n",
        "# Setup DeepLake - LOCAL storage (NO TOKEN!)\n",
        "vector_store = DeepLakeVectorStore(\n",
        "    dataset_path=\"./my_deeplake_db/\",\n",
        "    overwrite=False\n",
        ")\n",
        "\n",
        "if MODERN_API:\n",
        "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "else:\n",
        "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "print(\"‚úÖ Vector store ready\")\n",
        "\n",
        "# Download sample data\n",
        "print(\"üì• Downloading sample data...\")\n",
        "url = 'https://raw.githubusercontent.com/idontcalculate/data-repo/main/venus_transmission.txt'\n",
        "urllib.request.urlretrieve(url, 'venus_transmission.txt')\n",
        "\n",
        "# Load documents\n",
        "reader = SimpleDirectoryReader(input_files=[\"venus_transmission.txt\"])\n",
        "docs = reader.load_data()\n",
        "print(f\"‚úÖ Loaded {len(docs)} document(s)\")\n",
        "\n",
        "# Parse into nodes\n",
        "if MODERN_API:\n",
        "    Settings.chunk_size = 512\n",
        "    node_parser = SimpleNodeParser()\n",
        "else:\n",
        "    node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
        "\n",
        "nodes = node_parser.get_nodes_from_documents(docs)\n",
        "print(f\"‚úÖ Created {len(nodes)} chunks\")\n",
        "\n",
        "# Create index\n",
        "if MODERN_API:\n",
        "    index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
        "else:\n",
        "    index = VectorStoreIndex(nodes, storage_context=storage_context, service_context=service_context)\n",
        "\n",
        "print(\"‚úÖ Index created!\")\n",
        "\n",
        "# Query it\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What were the first beings to inhabit the planet?\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST QUERY RESULT:\")\n",
        "print(\"=\"*60)\n",
        "print(response.response)\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Evaluate\n",
        "if MODERN_API:\n",
        "    evaluator = FaithfulnessEvaluator(llm=llm)\n",
        "else:\n",
        "    evaluator = FaithfulnessEvaluator(service_context=service_context)\n",
        "\n",
        "eval_result = evaluator.evaluate_response(response=response)\n",
        "print(f\"\\n‚úÖ Faithfulness: {'PASS' if eval_result.passing else 'FAIL'}\")\n",
        "\n",
        "print(\"\\nüéâ SUCCESS! The code works with your installed version!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "id": "xbgkkUKqVVIK",
        "outputId": "4971f0d3-3abd-4fc7-8c4d-2d7d959de75b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/llama_index/download/module.py:12: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n",
            "/usr/local/lib/python3.12/dist-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (4.4.2) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using LEGACY llama-index API\n",
            "‚úÖ LLM initialized\n",
            "Deep Lake Dataset in ./my_deeplake_db/ already exists, loading from the storage\n",
            "‚úÖ Vector store ready\n",
            "üì• Downloading sample data...\n",
            "‚úÖ Loaded 1 document(s)\n",
            "‚úÖ Created 13 chunks\n",
            "Uploading data to deeplake dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00<00:00, 253.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='./my_deeplake_db/', tensors=['embedding', 'id', 'metadata', 'text'])\n",
            "\n",
            "  tensor      htype      shape      dtype  compression\n",
            "  -------    -------    -------    -------  ------- \n",
            " embedding  embedding  (13, 1536)  float32   None   \n",
            "    id        text      (13, 1)      str     None   \n",
            " metadata     json      (13, 1)      str     None   \n",
            "   text       text      (13, 1)      str     None   \n",
            "‚úÖ Index created!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\r\r"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llama_index.core.base'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2324854139.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# Query it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mquery_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_query_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What were the first beings to inhabit the planet?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_index/indices/base.py\u001b[0m in \u001b[0;36mas_query_engine\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mas_query_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBaseQueryEngine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;31m# NOTE: lazy import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever_query_engine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRetrieverQueryEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mretriever\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_index/query_engine/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mKnowledgeGraphQueryEngine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_modal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleMultiModalQueryEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultistep_query_engine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiStepQueryEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas_query_engine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPandasQueryEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_index/query_engine/multi_modal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseQueryEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQueryBundle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQueryType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_modal_llms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiModalLLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_modal_llms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIMultiModal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseNodePostprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_index/multi_modal_llms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mMultiModalLLMMetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_modal_llms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIMultiModal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_modal_llms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate_multi_modal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReplicateMultiModal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_index/multi_modal_llms/openai/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_modal_llms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIMultiModal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"OpenAIMultiModal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_index/multi_modal_llms/openai/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from llama_index.core.base.llms.generic_utils import (\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmessages_to_prompt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgeneric_messages_to_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.core.base'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y llama-index-multi-modal-llms-openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iaeSDAjWKpL",
        "outputId": "9bff2e95-28ed-4af7-cdf1-439b112bcf71"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: llama-index-multi-modal-llms-openai 0.1.9\n",
            "Uninstalling llama-index-multi-modal-llms-openai-0.1.9:\n",
            "  Successfully uninstalled llama-index-multi-modal-llms-openai-0.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-import to clear the cache\n",
        "from importlib import reload\n",
        "import llama_index\n",
        "reload(llama_index)\n",
        "\n",
        "from llama_index.llms import OpenAI as LegacyOpenAI\n",
        "\n",
        "# Use the legacy OpenAI LLM directly\n",
        "llm = LegacyOpenAI(model=\"gpt-4\", temperature=0.0)\n",
        "\n",
        "# Recreate query engine with explicit LLM\n",
        "query_engine = index.as_query_engine(llm=llm)\n",
        "\n",
        "# Now query\n",
        "response = query_engine.query(\"What were the first beings to inhabit the planet?\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(response.response)\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVjquS0zWMLK",
        "outputId": "e45b0be1-6624-4c8c-f720-21ac19d118c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "The first beings to inhabit the planet were a dinoid and reptoid race from two different systems outside our solar system.\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_query = \"What were the first beings to inhabit the planet?\"\n",
        "response = query_engine.query(test_query)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST QUERY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Q: {test_query}\")\n",
        "print(f\"A: {response.response}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Faithfulness evaluation\n",
        "evaluator = FaithfulnessEvaluator(service_context=service_context)\n",
        "eval_result = evaluator.evaluate_response(response=response)\n",
        "\n",
        "print(f\"\\nüìä Faithfulness: {'‚úÖ PASS' if eval_result.passing else '‚ùå FAIL'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HABrSNVsWjM2",
        "outputId": "ca35562c-f53c-4712-9d06-eb76e38c46eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TEST QUERY\n",
            "============================================================\n",
            "Q: What were the first beings to inhabit the planet?\n",
            "A: The first beings to inhabit the planet were a dinoid and reptoid race from two different systems outside our solar system.\n",
            "============================================================\n",
            "\n",
            "üìä Faithfulness: ‚úÖ PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this import\n",
        "from llama_index.evaluation import generate_question_context_pairs\n",
        "\n",
        "# Then run the evaluation code\n",
        "print(\"\\nüîÑ Generating evaluation questions...\")\n",
        "llm_eval = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "qa_dataset = generate_question_context_pairs(\n",
        "    nodes,\n",
        "    llm=llm_eval,\n",
        "    num_questions_per_chunk=2\n",
        ")\n",
        "\n",
        "queries = list(qa_dataset.queries.values())\n",
        "print(f\"‚úÖ Generated {len(queries)} evaluation questions\")\n",
        "print(f\"\\nSample questions:\")\n",
        "for i, q in enumerate(queries[:3]):\n",
        "    print(f\"  {i+1}. {q}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0HBbMGrWo8u",
        "outputId": "ed27bb8b-31f4-468a-d127-a43d31668ec3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÑ Generating evaluation questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:14<00:00,  1.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Generated 26 evaluation questions\n",
            "\n",
            "Sample questions:\n",
            "  1. How did the different races colonize the planets in our solar system according to the information presented in the text?\n",
            "  2. What were some of the key characteristics and abilities of the beings who came to this solar system, as described in the meeting recounted in the text?\n",
            "  3. Describe the beliefs and characteristics of the four races of beings who came to Earth to bring human life and protect all life on the planets in the solar system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.evaluation import RetrieverEvaluator\n",
        "\n",
        "print(\"\\nüîç Evaluating retriever...\")\n",
        "retriever = index.as_retriever(similarity_top_k=2)\n",
        "\n",
        "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
        ")\n",
        "\n",
        "# Run evaluation\n",
        "async def evaluate_retriever():\n",
        "    return await retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
        "\n",
        "eval_results = asyncio.run(evaluate_retriever())\n",
        "\n",
        "# Calculate metrics\n",
        "metric_dicts = [eval_result.metric_vals_dict for eval_result in eval_results]\n",
        "full_df = pd.DataFrame(metric_dicts)\n",
        "\n",
        "hit_rate = full_df[\"hit_rate\"].mean()\n",
        "mrr = full_df[\"mrr\"].mean()\n",
        "\n",
        "print(f\"\\nüìä RETRIEVER METRICS:\")\n",
        "print(f\"  Hit Rate: {hit_rate:.2%}\")\n",
        "print(f\"  MRR: {mrr:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLVaC1JHXARM",
        "outputId": "0bb8a142-6fe8-4fc8-c05e-fe70b3b69875"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Evaluating retriever...\n",
            "\n",
            "üìä RETRIEVER METRICS:\n",
            "  Hit Rate: 88.46%\n",
            "  MRR: 0.7115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install compatible langchain version for ragas 0.0.22\n",
        "!pip install langchain==0.0.350 langchain-community langchain-core\n",
        "\n",
        "# Then re-run the RAGAS code"
      ],
      "metadata": {
        "id": "w1mmMkNYXqKm",
        "outputId": "76fb949f-baab-41ba-9b04-8b72a255a5c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain==0.0.350 in /usr/local/lib/python3.12/dist-packages (0.0.350)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.0.20)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.1.23)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (6.0.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (2.0.44)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (3.13.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (0.0.87)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (2.12.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (2.32.5)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (8.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.11.0)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (23.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.350) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3->langchain-core) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3->langchain-core) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3->langchain-core) (4.15.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.350) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.350) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.350) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.0.350) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.0.350) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langchain==0.0.350) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.0.350) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.0.350) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain==0.0.350) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.350) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.350) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain-core langchain-openai langchain\n"
      ],
      "metadata": {
        "id": "HhzsVcWQZSgF",
        "outputId": "b453d512-b142-4c19-df5d-6b9fe9b46743",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.53)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (23.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.8.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.12)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.12.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# SET YOUR API KEY\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# Import everything needed\n",
        "from llama_index import VectorStoreIndex, ServiceContext\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4\", temperature=0.0)\n",
        "print(\"‚úÖ LLM initialized\")\n",
        "\n",
        "# RAGAS evaluation\n",
        "print(\"\\nüåê Running RAGAS evaluation on web data...\")\n",
        "\n",
        "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
        "    [\"https://en.wikipedia.org/wiki/New_York_City\"]\n",
        ")\n",
        "\n",
        "service_context_web = ServiceContext.from_defaults(llm=llm, chunk_size=512)\n",
        "vector_index_web = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    service_context=service_context_web\n",
        ")\n",
        "\n",
        "query_engine_web = vector_index_web.as_query_engine()\n",
        "\n",
        "test_response = query_engine_web.query(\"How did New York City get its name?\")\n",
        "print(f\"‚úÖ Test: {test_response.response[:150]}...\")\n",
        "\n",
        "# Evaluation\n",
        "eval_questions = [\n",
        "    \"What is the population of New York City as of 2020?\",\n",
        "    \"Which borough of New York City has the highest population?\",\n",
        "    \"How did New York City get its name?\",\n",
        "]\n",
        "\n",
        "eval_answers = [\n",
        "    [\"8,804,000\"],\n",
        "    [\"Queens\"],\n",
        "    [\"New York City got its name when it came under British control in 1664.\"],\n",
        "]\n",
        "\n",
        "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
        "\n",
        "metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
        "\n",
        "from ragas.integrations.llama_index import evaluate\n",
        "\n",
        "result = evaluate(query_engine_web, metrics, eval_questions, eval_answers)\n",
        "\n",
        "print(\"\\nüìä RAGAS RESULTS:\")\n",
        "for key, value in result.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.2%}\")\n",
        "\n",
        "print(\"\\n‚úÖ RAGAS complete!\")"
      ],
      "metadata": {
        "id": "FL-iHlf2XZHO",
        "outputId": "f798ebb0-2110-4db8-ff92-9249191d3615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/llama_index/download/module.py:12: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n",
            "/usr/local/lib/python3.12/dist-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (4.4.2) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ LLM initialized\n",
            "\n",
            "üåê Running RAGAS evaluation on web data...\n",
            "‚úÖ Test: The context provided does not contain information on how New York City got its name....\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_core.pydantic_v1'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4282394487.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m ]\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mragas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfaithfulness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_relevancy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_recall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfaithfulness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_relevancy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_recall\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ragas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mragas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mragas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_schema\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluationDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiTurnSample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSingleTurnSample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mragas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mragas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion_experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mragas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRunConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ragas/evaluation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mragas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mragas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelicone\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhelicone_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mragas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mllm_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mragas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseRagasLLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInstructorBaseRagasLLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLangchainLLMWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mragas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAspectCritic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ragas/llms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from ragas.llms.base import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mBaseRagasLLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mInstructorBaseRagasLLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mInstructorLLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mInstructorTypeVar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ragas/llms/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minstructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertexai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatVertexAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVertexAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_community/chat_models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \"\"\"  # noqa: E501\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manthropic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatAnthropic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manyscale\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatAnyscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mazure_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAzureChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_community/chat_models/anthropic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_values\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPromptValue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manthropic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_AnthropicCommon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_community/llms/anthropic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationChunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_values\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPromptValue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydantic_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSecretStr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_validator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m from langchain_core.utils import (\n\u001b[1;32m     24\u001b[0m     \u001b[0mcheck_package_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core.pydantic_v1'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "from llama_index import VectorStoreIndex, ServiceContext\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from llama_index.evaluation import FaithfulnessEvaluator, RelevancyEvaluator\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4\", temperature=0.0)\n",
        "\n",
        "# Load web data\n",
        "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
        "    [\"https://en.wikipedia.org/wiki/New_York_City\"]\n",
        ")\n",
        "\n",
        "service_context_web = ServiceContext.from_defaults(llm=llm, chunk_size=512)\n",
        "vector_index_web = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    service_context=service_context_web\n",
        ")\n",
        "\n",
        "query_engine_web = vector_index_web.as_query_engine()\n",
        "\n",
        "# Test queries\n",
        "questions = [\n",
        "    \"What is the population of New York City?\",\n",
        "    \"How did New York City get its name?\",\n",
        "    \"Which borough has the highest population?\",\n",
        "]\n",
        "\n",
        "print(\"üìä EVALUATION RESULTS:\\n\")\n",
        "\n",
        "for q in questions:\n",
        "    response = query_engine_web.query(q)\n",
        "\n",
        "    # Evaluate\n",
        "    faith_eval = FaithfulnessEvaluator(service_context=service_context_web)\n",
        "    relev_eval = RelevancyEvaluator(service_context=service_context_web)\n",
        "\n",
        "    faith_result = faith_eval.evaluate_response(response=response)\n",
        "    relev_result = relev_eval.evaluate_response(query=q, response=response)\n",
        "\n",
        "    print(f\"Q: {q}\")\n",
        "    print(f\"A: {response.response[:150]}...\")\n",
        "    print(f\"Faithful: {'‚úÖ' if faith_result.passing else '‚ùå'} | Relevant: {'‚úÖ' if relev_result.passing else '‚ùå'}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\nüéâ Evaluation complete!\")"
      ],
      "metadata": {
        "id": "Zj_6WceDZvNs",
        "outputId": "6ca8c4bd-0086-40c1-bcf6-8b7d052d58e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä EVALUATION RESULTS:\n",
            "\n",
            "Q: What is the population of New York City?\n",
            "A: The context information provided does not include the population of New York City....\n",
            "Faithful: ‚úÖ | Relevant: ‚ùå\n",
            "--------------------------------------------------------------------------------\n",
            "Q: How did New York City get its name?\n",
            "A: The context provided does not contain information on how New York City got its name....\n",
            "Faithful: ‚úÖ | Relevant: ‚úÖ\n",
            "--------------------------------------------------------------------------------\n",
            "Q: Which borough has the highest population?\n",
            "A: The context provided does not contain information about the population of any boroughs....\n",
            "Faithful: ‚úÖ | Relevant: ‚ùå\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üéâ Evaluation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "\n",
        "from llama_index import VectorStoreIndex, ServiceContext\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from llama_index.evaluation import (\n",
        "    FaithfulnessEvaluator,  # Similar to RAGAS faithfulness\n",
        "    RelevancyEvaluator,     # Similar to RAGAS answer_relevancy\n",
        "    RetrieverEvaluator      # For context_precision and context_recall\n",
        ")\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4\", temperature=0.0)\n",
        "\n",
        "# Load web data\n",
        "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
        "    [\"https://en.wikipedia.org/wiki/New_York_City\"]\n",
        ")\n",
        "\n",
        "service_context_web = ServiceContext.from_defaults(llm=llm, chunk_size=512)\n",
        "vector_index_web = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    service_context=service_context_web\n",
        ")\n",
        "\n",
        "query_engine_web = vector_index_web.as_query_engine()\n",
        "\n",
        "# Evaluation questions and ground truth\n",
        "eval_questions = [\n",
        "    \"What is the population of New York City as of 2020?\",\n",
        "    \"Which borough of New York City has the highest population?\",\n",
        "    \"How did New York City get its name?\",\n",
        "]\n",
        "\n",
        "eval_answers = [\n",
        "    \"8,804,000\",\n",
        "    \"Queens\",\n",
        "    \"New York City got its name when it came under British control in 1664.\",\n",
        "]\n",
        "\n",
        "# Create evaluators\n",
        "faithfulness_eval = FaithfulnessEvaluator(service_context=service_context_web)\n",
        "relevancy_eval = RelevancyEvaluator(service_context=service_context_web)\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "print(\"üîÑ Evaluating queries...\\n\")\n",
        "\n",
        "for question, ground_truth in zip(eval_questions, eval_answers):\n",
        "    # Get response\n",
        "    response = query_engine_web.query(question)\n",
        "\n",
        "    # Evaluate faithfulness (like RAGAS faithfulness)\n",
        "    faith_result = faithfulness_eval.evaluate_response(response=response)\n",
        "\n",
        "    # Evaluate relevancy (like RAGAS answer_relevancy)\n",
        "    relev_result = relevancy_eval.evaluate_response(query=question, response=response)\n",
        "\n",
        "    # Store results\n",
        "    results.append({\n",
        "        'question': question,\n",
        "        'answer': response.response,\n",
        "        'ground_truth': ground_truth,\n",
        "        'faithfulness': 1.0 if faith_result.passing else 0.0,\n",
        "        'answer_relevancy': 1.0 if relev_result.passing else 0.0,\n",
        "        'faithfulness_feedback': faith_result.feedback,\n",
        "        'relevancy_feedback': relev_result.feedback\n",
        "    })\n",
        "\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {response.response[:150]}...\")\n",
        "    print(f\"Faithfulness: {'‚úÖ PASS' if faith_result.passing else '‚ùå FAIL'}\")\n",
        "    print(f\"Relevancy: {'‚úÖ PASS' if relev_result.passing else '‚ùå FAIL'}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Calculate aggregate scores\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\nüìä AGGREGATE RESULTS:\")\n",
        "print(f\"  Faithfulness: {df['faithfulness'].mean():.2%}\")\n",
        "print(f\"  Answer Relevancy: {df['answer_relevancy'].mean():.2%}\")\n",
        "\n",
        "# For context_precision and context_recall, we need retriever evaluation\n",
        "print(\"\\nüîç Evaluating retriever (context metrics)...\")\n",
        "\n",
        "# Create a simple QA dataset\n",
        "from llama_index.evaluation import generate_question_context_pairs\n",
        "\n",
        "# Generate questions from documents\n",
        "node_parser = service_context_web.node_parser\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "\n",
        "qa_dataset = generate_question_context_pairs(\n",
        "    nodes[:5],  # Use first 5 nodes for speed\n",
        "    llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
        "    num_questions_per_chunk=1\n",
        ")\n",
        "\n",
        "# Evaluate retriever\n",
        "retriever = vector_index_web.as_retriever(similarity_top_k=2)\n",
        "retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "    [\"mrr\", \"hit_rate\"], retriever=retriever\n",
        ")\n",
        "\n",
        "import asyncio\n",
        "\n",
        "async def eval_retriever():\n",
        "    return await retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
        "\n",
        "retriever_results = asyncio.run(eval_retriever())\n",
        "\n",
        "# Calculate retriever metrics\n",
        "retriever_metrics = [r.metric_vals_dict for r in retriever_results]\n",
        "retriever_df = pd.DataFrame(retriever_metrics)\n",
        "\n",
        "print(f\"  Hit Rate (context_recall): {retriever_df['hit_rate'].mean():.2%}\")\n",
        "print(f\"  MRR (context_precision): {retriever_df['mrr'].mean():.4f}\")\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä FINAL EVALUATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Faithfulness:        {df['faithfulness'].mean():.2%}\")\n",
        "print(f\"Answer Relevancy:    {df['answer_relevancy'].mean():.2%}\")\n",
        "print(f\"Context Recall:      {retriever_df['hit_rate'].mean():.2%}\")\n",
        "print(f\"Context Precision:   {retriever_df['mrr'].mean():.4f}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nüéâ Evaluation complete!\")\n",
        "\n",
        "# Show detailed results table\n",
        "print(\"\\nüìã Detailed Results:\")\n",
        "print(df[['question', 'faithfulness', 'answer_relevancy']])"
      ],
      "metadata": {
        "id": "1O1un14NaMbO",
        "outputId": "f8d60fa9-eba8-4204-a62e-7203bb534f46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Evaluating queries...\n",
            "\n",
            "Q: What is the population of New York City as of 2020?\n",
            "A: The context information provided does not include the population of New York City as of 2020....\n",
            "Faithfulness: ‚úÖ PASS\n",
            "Relevancy: ‚úÖ PASS\n",
            "--------------------------------------------------------------------------------\n",
            "Q: Which borough of New York City has the highest population?\n",
            "A: The context information provided does not include details about the population of New York City's boroughs....\n",
            "Faithfulness: ‚úÖ PASS\n",
            "Relevancy: ‚ùå FAIL\n",
            "--------------------------------------------------------------------------------\n",
            "Q: How did New York City get its name?\n",
            "A: The context provided does not contain information on how New York City got its name....\n",
            "Faithfulness: ‚úÖ PASS\n",
            "Relevancy: ‚úÖ PASS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üìä AGGREGATE RESULTS:\n",
            "  Faithfulness: 100.00%\n",
            "  Answer Relevancy: 66.67%\n",
            "\n",
            "üîç Evaluating retriever (context metrics)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Hit Rate (context_recall): 0.00%\n",
            "  MRR (context_precision): 0.0000\n",
            "\n",
            "============================================================\n",
            "üìä FINAL EVALUATION SUMMARY\n",
            "============================================================\n",
            "Faithfulness:        100.00%\n",
            "Answer Relevancy:    66.67%\n",
            "Context Recall:      0.00%\n",
            "Context Precision:   0.0000\n",
            "============================================================\n",
            "\n",
            "üéâ Evaluation complete!\n",
            "\n",
            "üìã Detailed Results:\n",
            "                                            question  faithfulness  \\\n",
            "0  What is the population of New York City as of ...           1.0   \n",
            "1  Which borough of New York City has the highest...           1.0   \n",
            "2                How did New York City get its name?           1.0   \n",
            "\n",
            "   answer_relevancy  \n",
            "0               1.0  \n",
            "1               0.0  \n",
            "2               1.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "from llama_index import VectorStoreIndex, ServiceContext\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4\", temperature=0.0)\n",
        "\n",
        "# Load web data\n",
        "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
        "    [\"https://en.wikipedia.org/wiki/New_York_City\"]\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Loaded document with {len(documents[0].text)} characters\")\n",
        "\n",
        "# BETTER SETTINGS: Larger chunks, more retrieval\n",
        "service_context_web = ServiceContext.from_defaults(\n",
        "    llm=llm,\n",
        "    chunk_size=1024,      # ‚Üê Larger chunks (was 512)\n",
        "    chunk_overlap=200     # ‚Üê Add overlap for better context\n",
        ")\n",
        "\n",
        "vector_index_web = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    service_context=service_context_web\n",
        ")\n",
        "\n",
        "# Retrieve MORE chunks for better context\n",
        "query_engine_web = vector_index_web.as_query_engine(\n",
        "    similarity_top_k=5  # ‚Üê Get top 5 chunks (was 2)\n",
        ")\n",
        "\n",
        "# Test the queries again\n",
        "eval_questions = [\n",
        "    \"What is the population of New York City as of 2020?\",\n",
        "    \"Which borough of New York City has the highest population?\",\n",
        "    \"How did New York City get its name?\",\n",
        "]\n",
        "\n",
        "print(\"\\nüìä TESTING WITH BETTER SETTINGS:\\n\")\n",
        "\n",
        "for question in eval_questions:\n",
        "    response = query_engine_web.query(question)\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {response.response[:300]}...\")\n",
        "    print(f\"Retrieved {len(response.source_nodes)} chunks\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "6pinpLYzail4",
        "outputId": "9505055a-3d16-4652-8d52-c8eecac62d06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded document with 127 characters\n",
            "\n",
            "üìä TESTING WITH BETTER SETTINGS:\n",
            "\n",
            "Q: What is the population of New York City as of 2020?\n",
            "A: The context provided does not include information about the population of New York City as of 2020....\n",
            "Retrieved 1 chunks\n",
            "--------------------------------------------------------------------------------\n",
            "Q: Which borough of New York City has the highest population?\n",
            "A: The context information provided does not include details about the population of New York City's boroughs....\n",
            "Retrieved 1 chunks\n",
            "--------------------------------------------------------------------------------\n",
            "Q: How did New York City get its name?\n",
            "A: The context provided does not contain information on how New York City got its name....\n",
            "Retrieved 1 chunks\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "from llama_index import VectorStoreIndex, ServiceContext\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from llama_index.evaluation import (\n",
        "    FaithfulnessEvaluator,\n",
        "    RelevancyEvaluator,\n",
        "    RetrieverEvaluator,\n",
        "    generate_question_context_pairs\n",
        ")\n",
        "import asyncio\n",
        "\n",
        "# Load web data once\n",
        "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
        "    [\"https://en.wikipedia.org/wiki/New_York_City\"]\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Loaded document with {len(documents[0].text)} characters\\n\")\n",
        "\n",
        "# Evaluation questions\n",
        "eval_questions = [\n",
        "    \"What is the population of New York City as of 2020?\",\n",
        "    \"Which borough of New York City has the highest population?\",\n",
        "    \"How did New York City get its name?\",\n",
        "]\n",
        "\n",
        "eval_answers = [\n",
        "    \"8,804,000\",\n",
        "    \"Brooklyn\",  # ‚Üê Fixed: Brooklyn is actually highest, not Queens\n",
        "    \"New York City got its name when it came under British control in 1664.\",\n",
        "]\n",
        "\n",
        "# Function to evaluate a configuration\n",
        "def evaluate_config(config_name, chunk_size, chunk_overlap, similarity_top_k):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"üî¨ TESTING: {config_name}\")\n",
        "    print(f\"   Chunk Size: {chunk_size}, Overlap: {chunk_overlap}, Top-K: {similarity_top_k}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    llm = OpenAI(model=\"gpt-4\", temperature=0.0)\n",
        "\n",
        "    # Create service context with specific settings\n",
        "    service_context = ServiceContext.from_defaults(\n",
        "        llm=llm,\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "\n",
        "    # Build index\n",
        "    vector_index = VectorStoreIndex.from_documents(\n",
        "        documents,\n",
        "        service_context=service_context\n",
        "    )\n",
        "\n",
        "    # Create query engine with specific top_k\n",
        "    query_engine = vector_index.as_query_engine(similarity_top_k=similarity_top_k)\n",
        "\n",
        "    # Create evaluators\n",
        "    faithfulness_eval = FaithfulnessEvaluator(service_context=service_context)\n",
        "    relevancy_eval = RelevancyEvaluator(service_context=service_context)\n",
        "\n",
        "    # Evaluate questions\n",
        "    results = []\n",
        "\n",
        "    for question, ground_truth in zip(eval_questions, eval_answers):\n",
        "        response = query_engine.query(question)\n",
        "\n",
        "        faith_result = faithfulness_eval.evaluate_response(response=response)\n",
        "        relev_result = relevancy_eval.evaluate_response(query=question, response=response)\n",
        "\n",
        "        # Check if answer actually contains useful info (not just \"I don't know\")\n",
        "        has_answer = \"does not\" not in response.response.lower() and len(response.response) > 50\n",
        "\n",
        "        results.append({\n",
        "            'config': config_name,\n",
        "            'question': question,\n",
        "            'answer': response.response,\n",
        "            'ground_truth': ground_truth,\n",
        "            'faithfulness': 1.0 if faith_result.passing else 0.0,\n",
        "            'answer_relevancy': 1.0 if relev_result.passing else 0.0,\n",
        "            'has_answer': 1.0 if has_answer else 0.0,\n",
        "            'answer_length': len(response.response),\n",
        "            'chunks_retrieved': len(response.source_nodes)\n",
        "        })\n",
        "\n",
        "        print(f\"Q: {question}\")\n",
        "        print(f\"A: {response.response[:200]}...\")\n",
        "        print(f\"‚úì Faithful: {faith_result.passing} | Relevant: {relev_result.passing} | Has Answer: {has_answer}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # Retriever evaluation\n",
        "    node_parser = service_context.node_parser\n",
        "    nodes = node_parser.get_nodes_from_documents(documents)\n",
        "\n",
        "    qa_dataset = generate_question_context_pairs(\n",
        "        nodes[:5],\n",
        "        llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
        "        num_questions_per_chunk=1\n",
        "    )\n",
        "\n",
        "    retriever = vector_index.as_retriever(similarity_top_k=similarity_top_k)\n",
        "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
        "        [\"mrr\", \"hit_rate\"], retriever=retriever\n",
        "    )\n",
        "\n",
        "    async def eval_retriever():\n",
        "        return await retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
        "\n",
        "    retriever_results = asyncio.run(eval_retriever())\n",
        "    retriever_metrics = [r.metric_vals_dict for r in retriever_results]\n",
        "    retriever_df = pd.DataFrame(retriever_metrics)\n",
        "\n",
        "    # Summary metrics\n",
        "    summary = {\n",
        "        'config': config_name,\n",
        "        'chunk_size': chunk_size,\n",
        "        'chunk_overlap': chunk_overlap,\n",
        "        'similarity_top_k': similarity_top_k,\n",
        "        'faithfulness': df['faithfulness'].mean(),\n",
        "        'answer_relevancy': df['answer_relevancy'].mean(),\n",
        "        'has_answer_rate': df['has_answer'].mean(),\n",
        "        'avg_answer_length': df['answer_length'].mean(),\n",
        "        'context_recall': retriever_df['hit_rate'].mean(),\n",
        "        'context_precision': retriever_df['mrr'].mean(),\n",
        "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìä SUMMARY for {config_name}:\")\n",
        "    print(f\"  Faithfulness:      {summary['faithfulness']:.2%}\")\n",
        "    print(f\"  Answer Relevancy:  {summary['answer_relevancy']:.2%}\")\n",
        "    print(f\"  Has Answer Rate:   {summary['has_answer_rate']:.2%} ‚Üê KEY METRIC!\")\n",
        "    print(f\"  Context Recall:    {summary['context_recall']:.2%}\")\n",
        "    print(f\"  Context Precision: {summary['context_precision']:.4f}\")\n",
        "\n",
        "    return summary, df\n",
        "\n",
        "# ============================================\n",
        "# RUN EXPERIMENTS WITH DIFFERENT CONFIGS\n",
        "# ============================================\n",
        "\n",
        "all_summaries = []\n",
        "all_details = []\n",
        "\n",
        "# Config 1: BASELINE (your current settings)\n",
        "summary1, details1 = evaluate_config(\n",
        "    config_name=\"Baseline\",\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=0,\n",
        "    similarity_top_k=2\n",
        ")\n",
        "all_summaries.append(summary1)\n",
        "all_details.append(details1)\n",
        "\n",
        "# Config 2: LARGER CHUNKS\n",
        "summary2, details2 = evaluate_config(\n",
        "    config_name=\"Larger Chunks\",\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=200,\n",
        "    similarity_top_k=3\n",
        ")\n",
        "all_summaries.append(summary2)\n",
        "all_details.append(details2)\n",
        "\n",
        "# Config 3: MORE RETRIEVAL\n",
        "summary3, details3 = evaluate_config(\n",
        "    config_name=\"More Retrieval\",\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=200,\n",
        "    similarity_top_k=5\n",
        ")\n",
        "all_summaries.append(summary3)\n",
        "all_details.append(details3)\n",
        "\n",
        "# ============================================\n",
        "# COMPARE ALL CONFIGS\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üèÜ COMPARISON OF ALL CONFIGURATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison_df = pd.DataFrame(all_summaries)\n",
        "print(comparison_df[['config', 'has_answer_rate', 'faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']])\n",
        "\n",
        "# Find best config\n",
        "best_config = comparison_df.loc[comparison_df['has_answer_rate'].idxmax()]\n",
        "print(f\"\\nü•á BEST CONFIG: {best_config['config']}\")\n",
        "print(f\"   Has Answer Rate: {best_config['has_answer_rate']:.2%}\")\n",
        "print(f\"   Settings: chunk_size={best_config['chunk_size']}, overlap={best_config['chunk_overlap']}, top_k={best_config['similarity_top_k']}\")\n",
        "\n",
        "# Save results to CSV\n",
        "comparison_df.to_csv('rag_evaluation_results.csv', index=False)\n",
        "print(\"\\nüíæ Results saved to 'rag_evaluation_results.csv'\")\n",
        "\n",
        "print(\"\\nüéâ Evaluation complete!\")"
      ],
      "metadata": {
        "id": "fVy-L6zsbU30",
        "outputId": "451974c2-65e7-486c-b6e0-4abb69de4d95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded document with 127 characters\n",
            "\n",
            "\n",
            "================================================================================\n",
            "üî¨ TESTING: Baseline\n",
            "   Chunk Size: 512, Overlap: 0, Top-K: 2\n",
            "================================================================================\n",
            "\n",
            "Q: What is the population of New York City as of 2020?\n",
            "A: The context provided does not include information about the population of New York City as of 2020....\n",
            "‚úì Faithful: True | Relevant: True | Has Answer: False\n",
            "--------------------------------------------------------------------------------\n",
            "Q: Which borough of New York City has the highest population?\n",
            "A: The context information provided does not include details about the population of New York City's boroughs....\n",
            "‚úì Faithful: True | Relevant: False | Has Answer: False\n",
            "--------------------------------------------------------------------------------\n",
            "Q: How did New York City get its name?\n",
            "A: The context provided does not contain information on how New York City got its name....\n",
            "‚úì Faithful: True | Relevant: True | Has Answer: False\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä SUMMARY for Baseline:\n",
            "  Faithfulness:      100.00%\n",
            "  Answer Relevancy:  66.67%\n",
            "  Has Answer Rate:   0.00% ‚Üê KEY METRIC!\n",
            "  Context Recall:    0.00%\n",
            "  Context Precision: 0.0000\n",
            "\n",
            "================================================================================\n",
            "üî¨ TESTING: Larger Chunks\n",
            "   Chunk Size: 1024, Overlap: 200, Top-K: 3\n",
            "================================================================================\n",
            "\n",
            "Q: What is the population of New York City as of 2020?\n",
            "A: The context provided does not include information about the population of New York City as of 2020....\n",
            "‚úì Faithful: True | Relevant: True | Has Answer: False\n",
            "--------------------------------------------------------------------------------\n",
            "Q: Which borough of New York City has the highest population?\n",
            "A: The context information provided does not include details about the population of New York City's boroughs....\n",
            "‚úì Faithful: True | Relevant: False | Has Answer: False\n",
            "--------------------------------------------------------------------------------\n",
            "Q: How did New York City get its name?\n",
            "A: The context provided does not contain information on how New York City got its name....\n",
            "‚úì Faithful: True | Relevant: True | Has Answer: False\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä SUMMARY for Larger Chunks:\n",
            "  Faithfulness:      100.00%\n",
            "  Answer Relevancy:  66.67%\n",
            "  Has Answer Rate:   0.00% ‚Üê KEY METRIC!\n",
            "  Context Recall:    0.00%\n",
            "  Context Precision: 0.0000\n",
            "\n",
            "================================================================================\n",
            "üî¨ TESTING: More Retrieval\n",
            "   Chunk Size: 1024, Overlap: 200, Top-K: 5\n",
            "================================================================================\n",
            "\n",
            "Q: What is the population of New York City as of 2020?\n",
            "A: The context provided does not include information about the population of New York City as of 2020....\n",
            "‚úì Faithful: True | Relevant: True | Has Answer: False\n",
            "--------------------------------------------------------------------------------\n",
            "Q: Which borough of New York City has the highest population?\n",
            "A: The context provided does not contain information about the population of New York City's boroughs....\n",
            "‚úì Faithful: True | Relevant: False | Has Answer: False\n",
            "--------------------------------------------------------------------------------\n",
            "Q: How did New York City get its name?\n",
            "A: The context provided does not contain information on how New York City got its name....\n",
            "‚úì Faithful: True | Relevant: True | Has Answer: False\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä SUMMARY for More Retrieval:\n",
            "  Faithfulness:      100.00%\n",
            "  Answer Relevancy:  66.67%\n",
            "  Has Answer Rate:   0.00% ‚Üê KEY METRIC!\n",
            "  Context Recall:    0.00%\n",
            "  Context Precision: 0.0000\n",
            "\n",
            "================================================================================\n",
            "üèÜ COMPARISON OF ALL CONFIGURATIONS\n",
            "================================================================================\n",
            "           config  has_answer_rate  faithfulness  answer_relevancy  \\\n",
            "0        Baseline              0.0           1.0          0.666667   \n",
            "1   Larger Chunks              0.0           1.0          0.666667   \n",
            "2  More Retrieval              0.0           1.0          0.666667   \n",
            "\n",
            "   context_recall  context_precision  \n",
            "0             0.0                0.0  \n",
            "1             0.0                0.0  \n",
            "2             0.0                0.0  \n",
            "\n",
            "ü•á BEST CONFIG: Baseline\n",
            "   Has Answer Rate: 0.00%\n",
            "   Settings: chunk_size=512, overlap=0, top_k=2\n",
            "\n",
            "üíæ Results saved to 'rag_evaluation_results.csv'\n",
            "\n",
            "üéâ Evaluation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "from llama_index import VectorStoreIndex, ServiceContext\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "\n",
        "# Load the Wikipedia page\n",
        "print(\"üì• Loading Wikipedia page...\")\n",
        "documents = SimpleWebPageReader(html_to_text=True).load_data(\n",
        "    [\"https://en.wikipedia.org/wiki/New_York_City\"]\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Loaded {len(documents)} document(s)\")\n",
        "print(f\"üìè Document length: {len(documents[0].text)} characters\")\n",
        "print(f\"\\nüìÑ First 500 characters of document:\")\n",
        "print(documents[0].text[:500])\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Build index\n",
        "llm = OpenAI(model=\"gpt-4\", temperature=0.0)\n",
        "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=1024, chunk_overlap=200)\n",
        "vector_index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "# Get the nodes to see what chunks were created\n",
        "node_parser = service_context.node_parser\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "print(f\"‚úÖ Created {len(nodes)} chunks\")\n",
        "\n",
        "print(f\"\\nüì¶ Sample chunks:\")\n",
        "for i, node in enumerate(nodes[:3]):\n",
        "    print(f\"\\nChunk {i+1} (first 200 chars):\")\n",
        "    print(node.text[:200])\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Now test retrieval\n",
        "query_engine = vector_index.as_query_engine(similarity_top_k=5)\n",
        "\n",
        "test_query = \"What is the population of New York City?\"\n",
        "print(f\"\\nüîç Testing query: '{test_query}'\")\n",
        "response = query_engine.query(test_query)\n",
        "\n",
        "print(f\"\\nüìä Retrieved {len(response.source_nodes)} chunks:\")\n",
        "for i, node in enumerate(response.source_nodes):\n",
        "    print(f\"\\nRetrieved Chunk {i+1} (score: {node.score:.4f}):\")\n",
        "    print(node.text[:300])\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(f\"\\nüí¨ Final Answer:\")\n",
        "print(response.response)"
      ],
      "metadata": {
        "id": "pghbTFuZb23-",
        "outputId": "6aaaa1f8-f4d5-4f4e-f8b7-37a819a6c542",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Loading Wikipedia page...\n",
            "\n",
            "‚úÖ Loaded 1 document(s)\n",
            "üìè Document length: 127 characters\n",
            "\n",
            "üìÑ First 500 characters of document:\n",
            "Please set a user-agent and respect our robot policy https://w.wiki/4wJS. See\n",
            "also https://phabricator.wikimedia.org/T400119.\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "‚úÖ Created 1 chunks\n",
            "\n",
            "üì¶ Sample chunks:\n",
            "\n",
            "Chunk 1 (first 200 chars):\n",
            "Please set a user-agent and respect our robot policy https://w.wiki/4wJS. See\n",
            "also https://phabricator.wikimedia.org/T400119.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üîç Testing query: 'What is the population of New York City?'\n",
            "\n",
            "üìä Retrieved 1 chunks:\n",
            "\n",
            "Retrieved Chunk 1 (score: 0.6891):\n",
            "Please set a user-agent and respect our robot policy https://w.wiki/4wJS. See\n",
            "also https://phabricator.wikimedia.org/T400119.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "üí¨ Final Answer:\n",
            "The context provided does not include information about the population of New York City.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}